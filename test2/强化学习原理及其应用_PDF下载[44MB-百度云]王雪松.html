强化学习原理及其应用 PDF下载 王雪松 百度云 电子书 下载 电子书下载
PDF电子书下载不求人，看这篇文章就够了→ http://www.chendianrong.com/pdf#703040640
PDF电子书下载不求人，看这篇文章就够了→ http://www.chendianrong.com/pdf#703040640
<p>书名:强化学习原理及其应用</p><p>作者:王雪松</p><p>页数:253</p><p>定价:¥89.0</p><p>出版社:科学出版社</p><p>出版日期:2014-06-01</p><p>ISBN:9787030406408</p><p><h2>本书特色</h2></p>[<p>
书围绕着克服维数灾难，分别从值函数逼近、直接策略搜索和基于谱方法的学习3个方面来阐述强化学习理论、方法及其应用，《强化学习原理及其应用》是著者近5年来在国家自然科学基金、教育部"新世纪优秀人才支持计划"、江苏省自然科学基
                                        </p>]<p><h2>目录</h2></p>
《智能科学技术著作丛书》序序前言第1章 强化学习概述  1.1 强化学习模型及其基本要素    1.1.1 强化学习模型    1.1.2 强化学习基本要素  1.2 强化学习的发展历史    1.2.1 试错学习    1.2.2 动态规划与*优控制    1.2.3 时间差分学习  1.3 强化学习研究概述    1.3.1 分层强化学习研究现状    1.3.2 近似强化学习研究现状    1.3.3 启发式回报函数设计研究现状    1.3.4 探索和利用平衡研究现状    1.3.5 基于谱图理论的强化学习研究现状  1.4 强化学习方法的应用    1.4.1 自适应优化控制中的应用    1.4.2 调度管理中的应用    1.4.3 人工智能问题求解中的应用  1.5 本书主要内容及安排    参考文献第2章 强化学习基础理论  2.1 马尔科夫决策过程概述    2.1.1 马尔科夫决策过程    2.1.2 策略和值函数t  2.2 基于模型的动态规划方法    2.2.1 线性规划    2.2.2 策略迭代    2.2.3 值迭代    2.2.4 广义策略迭代  2.3 模型未知的强化学习    2.3.1 强化学习基础    2.3.2 蒙特卡罗法    2.3.3 时间差分td法    2.3.4 q学习与sarsa学习    2.3.5 dyna，学习框架    2.3.6 直接策略方法    2.3.7 actor-critic学习  2.4 近似强化学习    2.4.1 带值函数逼近的td学习    2.4.2 近似值迭代    2.4.3 近似策略迭代    2.4.4 *小二乘策略迭代  2.5 本章小结    参考文献第3章 基于支持向量机的强化学习  3.1 支持向量机原理    3.1.1 机器学习    3.1.2 核学习    3.1.3 svm的思想    3.1.4 svm的重要概念  3.2 基于半参数支持向量机的强化学习    3.2.1 基于半参数回归模型的q学习结构    3.2.2 半参数回归模型的学习    3.2.3 仿真研究  3.3 基于概率型支持向量机的强化学习    3.3.1 基于概率型支持向量机分类机的q学习    3.3.2 概率型支持向量分类机    3.3.3 仿真研究  3.4 本章小结    参考文献第4章 基于状态一动作图测地高斯基的策略迭代强化学习  4.1 强化学习中的基函数选择  4.2 基于状态一动作图测地高斯基的策略迭代    4.2.1 mdp的状态一动作空间图    4.2.2 状态一动作图上测地高斯核    4.2.3 基于状态一动作图测地高斯基的动作值函数逼近  4.3 算法步骤  4.4 仿真研究  4.5 本章小结    参考文献第5章 基于抽象状态的贝叶斯强化学习电梯群组调度  5.1 电梯群组调度强化学习模型  5.2 基于抽象状态的贝叶斯强化学习电梯群组调度    5.2.1 吠态空间抽象    5.2.2 强化学习系统的回报函数    5.2.3 贝叶斯网推断    5.2.4 状态-动作值函数的神经网络逼近    5.2.5 动作选择策略  5.3 仿真研究  5.4 本章小结    参考文献第6章 基于增量*小二乘时间差分的actor-critic学习  6.1 策略梯度理论  6.2 基于常规梯度的增量式actor—critic学习  6.3 基于ilstd(a)的actor—critic学习  6.4 仿真研究  6.5 本章小结    参考文献第7章 融合经验数据的actor.critic强化学习  7.1 增量式actor—critic学习算法的数据有效性改进    7.1.1 基于r15std(λ)或ilstd(λ)的增量式actor-critic学习    7.1.2 算法步骤    7.1.3 仿真研究  7.2 基于自适应重要采样的actor一critic学习    7.2.1 基于*小二乘时间差分的actor-critic强化学习    7.2.2 基于重要采样的估计    7.2.3 基于自适应重要采样的估计    7.2.4 算法步骤    7.2.5 仿真研究  7.3 本章小结    参考文献第8章 基于资格迹的折扣回报型增量自然actor-critic学习  8.1 自然梯度  8.2 自然策略梯度的估计方法    8.2.1 基于：fisher-信息矩阵的自然策略梯度    8.2.2 基于兼容函数逼近器的自然策略梯度    8.2.3 自然策略梯度的仿真    8.2.4 自然策略梯度的特性  8.3 基于资格迹的折扣回报型增量自然.actor—critic学习  8.4 仿真研究  8.5 本章小结    参考文献第9章 基于参数探索的em策略搜索  9.1 策略搜索强化学习方法分析  9.2 期望*大化策略搜索强化学习  9.3 基于参数探索的em策略搜索学习  9.4 算法步骤  9.5 仿真研究    9.5.1 小球平衡问题    9.5.2 倒立摆平衡问题  9.6 本章小结    参考文献第10章 基于谱图理论的强化学习基础  10.1.谱图理论与谱图分割    10.1.1 谱图理论与谱方法    10.1.2 谱图分割和谱聚类  10.2 基于谱图理论的流形和距离度量学习    10.2.1 流形学习概述    10.2.2 基于流形学习的度量学习  10.3 基于拉普拉斯特征映射法的强化学习    10.3.1 拉普拉斯特征映射法基础    10.3.2 基于拉普拉斯特征映射的强化学习  10.4 基于拉普拉斯特征映射的强化学习分析  10.5 本章小结    参考文献第11章 基于拉普拉斯特征映射的启发式策略选择  11.1 探索和利用平衡问题概述  11.2 启发式策略选择原理  11.3 基于拉普拉斯特征映射的启发式策略选择    11.3.1 基本思想    11.3.2 基于拉普拉斯特征映射的启发式q学习  11.4 算法步骤、计算复杂度和适用范围    11.4.1 算法主要步骤    11.4.2 计算复杂度    11.4.3 适用范围  11.5 仿真研究    11.5.1 5房间格子世界    11.5.2 对称4房间格子世界   1.6 本章小结    参考文献第12章 基于拉普拉斯特征映射的dyna规划  12.1 强化学习在移动机器人自主导航中的应用研究概述  12.2 强化学习在井下救援机器人导航中的应用研究  12.3 基于拉普拉斯特征映射的dynaq算法    12.3.1 dyna_q的基本思想    12.3.2 基于谱图理论的优先级机制    12.3.3 算法步骤    12.3.4 计算复杂度分析和适用范围  12.4 仿真结果及分析    12.4.1 5房间格子地图    12.4.2 对称4房间格子地图    12.4.3 9房间格子地图  12.5 本章小结    参考文献第13章 基于谱方法的强化学习迁移研究  13.1 基于谱图理论的强化学习迁移    13.1.1 强化学习迁移概述    13.1.2 基于谱图理论的强化学习迁移分析  13.2 基于谱图理论的option自动生成研究    13.2.1 option原理    13.2.2 基于谱图分割的option自动生成算法概述    13.2.3 虚拟值函数法  13.3 基于谱图理论的强化学习混合迁移方法    13.3.1 基函数的线性插值    13.3.2 迁移基函数的逼近能力    13.3.3 基函数与子任务策略的混合迁移  13.4 算法步骤和适用范围    13.4.1 算法步骤    13.4.2 适用范围  13.5 仿真实验与分析    13.5.1 地图不变迁移    13.5.2 地图比例放大迁移    13.5.3 实验结果统计分析  13.6 本章小结参考文献附录
